{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time has come to learn your first real model: Naive Bayes. The reason we're introducing this model first is because, actually, we already introduced everything you need to know a while ago. You should be familiar with __Bayes Rule__, which we covered in the fundamentals course (if you've forgotten don't worry, we'll cover it again briefly here). Naive Bayes is simply modeling and prediction based around this theorem.\n",
    "\n",
    "Let's approach this by thinking about Naive Bayes in terms of the two words in its name: Naive and Bayes.\n",
    "\n",
    "## Bayes\n",
    "\n",
    "Let's discuss Bayes first, since that's the core of the model. Bayes Theorem covers the probabilistic relationship between multiple variables, and specifically allows us to define one conditional in terms of the underlying probabilities and the inverse condition. Specifically, it can be defined as:\n",
    "\n",
    "$$P(y|x) = P(y)P(x|y)/P(x)$$\n",
    "\n",
    "In English this reads as \"the probability of y given x equals the probability of y times the probability of x given y divided by the probability of x.\"\n",
    "\n",
    "This theorem can be extended to when x is a vector (containing the multiple x variables used as inputs for the model) to:\n",
    "\n",
    "$$P(y|x_1,...,x_n) = P(y)P(x_1,...,x_n|y)/P(x_1,...,x_n)$$\n",
    "\n",
    "This explains the relationship of an outcome to a vector of conditions rather than to a single other event. Recall that this can be read as the probability of y, in the case of our model the categorical outcome weâ€™re interested in, given a set of observations is equal to the probability of that set of observations given y divided by the probability of that set of outcomes.\n",
    "\n",
    "We'll return to this probability later to define our model.\n",
    "\n",
    "## Naive\n",
    "\n",
    "The other part of Naive Bayes is of course Naive. In this setting Naive refers to the assumption that any pair of variables in the conditional vector (the x variables) are independent from each other.\n",
    "\n",
    "That independence does something really important to the vectorized Bayes Rule equation (the second one from above). It allows us to break that large $P(x_1,...,x_n|y)$ into the product of each individual condition. Written out it would be something like:\n",
    "\n",
    "$$P(y|x_1,...,x_n) = P(y)*P(x_1|y)*...*P(x_n|y)/P(x_1,...,x_n)$$\n",
    "\n",
    "We can even simplify further because for any observation we are attempting to predict, the x-vector will be constant, so that part of the probability simplifies out leaving:\n",
    "\n",
    "$$P(y|x_1,...,x_n) \\approx P(y)*P(x_1|y)*...*P(x_n|y)$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\\hat{y} = argmax_y(P(y)\\prod_{i=1}^nP(x_i|y))$$\n",
    "\n",
    "This states that our estimator of y is the maximum over y of the $P(y)*\\prod_{i=1}^nP(x_i|y)$.\n",
    "\n",
    "This is the basis for Naive Bayes as a model. As you can tell from this formula, Naive Bayes returns the y value that maximizes the following argument. This means it returns a single value, or category. \n",
    "\n",
    "Returning to the fact that our estimate is the y that maximizes the argument, this is because Naive Bayes is used as a classifier. We are interested in which y value is most likely to have given the observed set of x variables based on their Bayesian probabilities. There are ways to jigger the rule into returning probabilities, but these are generally NOT very accurate and not to be used. It is said that this is a good classifier but not a good estimator.\n",
    "\n",
    "\n",
    "## Simple is Sometimes better\n",
    "\n",
    "Now, we made some huge assumptions here to get to this predictor. The assumption of independence between each pair is hugely important and rarely totally accurate. The columns of your data frame are typically not independent of each other. \n",
    "\n",
    "However, Naive Bayes is still used in the real world.\n",
    "\n",
    "There are a few different reasons for that. Firstly, it is delightfully simple. It is easy to understand both how it operates and what it's doing. More importantly it is incredibly fast. That speed can occasionally be very useful from a practical perspective. It also relies on probabilities, which are based on counts, so you can actually train the classifier with more data than could fit into memory at one time (and scikit-learn even has an option to do this). That count reliance also contributes to its computational simplicity.\n",
    "\n",
    "There are also specific situations where Naive Bayes has been known to perform reasonably well. This is most common in sentiment classification, a branch of machine learning that is designed to focus on trying to classify textual samples according to sentiment. Practically it is very good for spam filtering or telling if comments are positive or negative.\n",
    "\n",
    "We'll make a Bayesian spam filter later in this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
