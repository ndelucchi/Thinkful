{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RFC_with_spark_in_batch_mode.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"colab_type":"text","id":"xNtRffCspUDK"},"cell_type":"markdown","source":["# Building a Random Forest Classifier with Spark in Batch Mode"]},{"metadata":{"colab_type":"text","id":"losewPCFpUDM"},"cell_type":"markdown","source":["In this notebook, you'll learn the basics of working with Spark in batch mode to build a random forest classifier. Before digging in, make sure to read the background context in the related reading in the curriculum!"]},{"metadata":{"colab_type":"text","id":"qqIBhMHQKY-x"},"cell_type":"markdown","source":["## Install Java, Spark, Findspark and PySpark\n","\n","First, you need to install Java and Apache Spark to the server that is allocated to you. The following code installs Apache Spark 2.4.0 and Java 8:"]},{"metadata":{"colab_type":"code","id":"QD--ZWt1Kqbs","colab":{}},"cell_type":"code","source":["!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q http://apache.osuosl.org/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n","!tar xf spark-2.4.0-bin-hadoop2.7.tgz"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"9xezq2UzMe0I"},"cell_type":"markdown","source":["Second, you need to set the locations where Spark and Java are installed:"]},{"metadata":{"colab_type":"code","id":"i-FWNdmmMiPR","colab":{}},"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\""],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"QXbaGP2OMk0H"},"cell_type":"markdown","source":["Third, you need to install [Findspark](https://github.com/minrk/findspark) (a library that makes it easy for Python to find Spark) and PySpark using pip:"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1549802433594,"user_tz":-180,"elapsed":44983,"user":{"displayName":"Yunus Bulut","photoUrl":"","userId":"10252913168794832935"}},"id":"MLU-t6XpMnxK","outputId":"d1420fbd-72d0-4d21-ba78-b00a90654a21","colab":{"base_uri":"https://localhost:8080/","height":241}},"cell_type":"code","source":["!pip install -q findspark\n","!pip install pyspark"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting pyspark\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/01/a37e827c2d80c6a754e40e99b9826d978b55254cc6c6672b5b08f2e18a7f/pyspark-2.4.0.tar.gz (213.4MB)\n","\u001b[K    100% |████████████████████████████████| 213.4MB 108kB/s \n","\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n","\u001b[K    100% |████████████████████████████████| 204kB 29.6MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/cd/54/c2/abfcc942eddeaa7101228ebd6127a30dbdf903c72db4235b23\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.7 pyspark-2.4.0\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"2Htjax0nMsbK"},"cell_type":"markdown","source":["Last, you need to mount your Google Drive so that you can access the files on your drive. This will enable you to read from the data files you uploaded to your drive:"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1549802437130,"user_tz":-180,"elapsed":900,"user":{"displayName":"Yunus Bulut","photoUrl":"","userId":"10252913168794832935"}},"id":"jQcf-Qv-MxED","outputId":"9c155259-70a4-446c-d3fa-579007fd1f44","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"T8HNvqY4pUDO"},"cell_type":"markdown","source":["##  Import dependencies\n","\n","Now, we need to import the tools we'll need from PySpark. The imports below allow us to connect to the Spark server, load our data, clean it, and prepare, execute, and evaluate a model."]},{"metadata":{"colab_type":"code","id":"zt14E2h8pUDQ","colab":{}},"cell_type":"code","source":["from pyspark import SparkContext\n","from pyspark.sql import SparkSession\n","\n","from pyspark.ml import Pipeline\n","from pyspark.ml.classification import RandomForestClassifier\n","from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","\n","from pyspark.sql.functions import isnan, when, count, col"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"q3o8-3-MpUDZ"},"cell_type":"markdown","source":["## Set our constants\n","\n","Next, we create a set of constants that we can refer to throughout the notebook. These are values that the rest of our code needs to run, but that we might need to change at some point (for instance, if the location of our data changes). "]},{"metadata":{"colab_type":"code","id":"GHuCjb58pUDb","colab":{}},"cell_type":"code","source":["CSV_PATH = \"/content/gdrive/My Drive/Colab Datasets/allData.csv\"\n","CSV_ACTIVITY_LABEL_PATH = \"/content/gdrive/My Drive/Colab Datasets/activity_labels.csv\"\n","APP_NAME = \"UCI HAR Random Forest Example\"\n","SPARK_URL = \"local[*]\"\n","RANDOM_SEED = 141107\n","TRAINING_DATA_RATIO = 0.8\n","RF_NUM_TREES = 10\n","RF_MAX_DEPTH = 4\n","RF_NUM_BINS = 32"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"lz2fYFpFpUDg"},"cell_type":"markdown","source":["## Connect to the server and load data\n","\n","Now we're ready to connect to the Spark server. We do that (relying on the constants set above) and then load our labels (loaded into `activity_labels`) and activity data (loaded into `df`). "]},{"metadata":{"colab_type":"code","id":"f0E5G4QgpUDi","colab":{}},"cell_type":"code","source":["spark = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()\n","activity_labels = spark.read.options(inferschema = \"true\").csv(CSV_ACTIVITY_LABEL_PATH)\n","df = spark.read.options(inferschema = \"true\").csv(CSV_PATH)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"2rEPfpSepUDn"},"cell_type":"markdown","source":["## Validate the data\n","\n","If our data has been properly cleaned and prepared, it will meet the following criteria, which we'll verify in just a moment:\n","\n","* The dataframe shape should be 10,299 rows by 562 columns\n","* All feature columns should be doubles. Note than one of the columns is for our label and it will not be double.\n","* There should be no nulls. This point is crucial because Spark will fail to build our vector variables for our classifier if there are any null values.\n","\n","Let's confirm these points."]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1549802473840,"user_tz":-180,"elapsed":2268,"user":{"displayName":"Yunus Bulut","photoUrl":"","userId":"10252913168794832935"}},"id":"rtoz5LXGpUDo","outputId":"b91536e1-7332-4d37-b26c-f9c9e6566e6e","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Confirm the dataframe shape is 10,299 rows by 562 columns\n","print(f\"Dataset shape is {df.count():d} rows by {len(df.columns):d} columns.\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Dataset shape is 10299 rows by 562 columns.\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1549802475010,"user_tz":-180,"elapsed":479,"user":{"displayName":"Yunus Bulut","photoUrl":"","userId":"10252913168794832935"}},"id":"gAIAHlM_pUDy","outputId":"e00cf51a-0d31-4800-88d0-b1e89f528b88","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Confirm that all feature columns are doubles via a list comprehension\n","# We're expecting 561 of 562 here, accounting for the labels column\n","double_cols = [col[0] for col in df.dtypes if col[1] == 'double']\n","print(f\"{len(double_cols):d} columns out of {len(df.columns):d} total are type double.\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["561 columns out of 562 total are type double.\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1549802497096,"user_tz":-180,"elapsed":20129,"user":{"displayName":"Yunus Bulut","photoUrl":"","userId":"10252913168794832935"}},"id":"qgdEDXW5pUD6","outputId":"599ce78c-ae6b-47f3-fe06-ef57a898af11","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Confirm there are no null values. We use the dataframe select method to build a \n","# list that is then converted to a Python dict. This way it's easy to sum up the nulls.\n","null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \n","                         for c in df.columns]).toPandas().to_dict(orient='records')\n","\n","print(f\"There are {sum(null_counts[0].values()):d} null values in the dataset.\")"],"execution_count":10,"outputs":[{"output_type":"stream","text":["There are 0 null values in the dataset.\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"LFFVa285pUEB"},"cell_type":"markdown","source":["## Set up and run our classifier in Spark\n","\n","After confirming our data is clean, we're ready to reshape the data and run the random forest model.\n","\n","In Spark, we manipulate the data to work in a Spark pipeline, define each of the steps in the pipeline, chain them together, and finally run the pipeline.\n","\n","Apache Spark classifiers expect 2 columns of input:\n","\n","1. __labels__: an indexed set of numeric variables that represent the classification from the set of features we provide.\n","2. __features__: an indexed, vector variable that contains all of the feature values in each row. \n","\n","In order to do this, we need to create these 2 columns from our dataset - the data is there, but not yet in a format we can use in the classifier.\n","\n","To create the indexed labels column, we'll create a column called `indexedLabel` using the `StringIndexer` method. We use the column `_c0` as the source for our label index since that contains our labels. The column contains only one value per index.\n","    \n","To create the indexed features column, we'll need to do two things. First, we'll create the vector of features using the `VectorAssembler` method. To create this vector, we'll need to use all 561 numeric columns from our data frame. The vector assembler will create a new column called `features`, and each row of this column will contain a 561-element vector that is built from the 561 features in the dataset.\n","\n","Finally, we'll complete the data preparation by creating an indexed vector from the `features` column. We'll call this vector `indexedFeatures`.\n","    \n","Since the classifier expects indexed labels and an indexed vector column of data, we'll use the `indexedLabel` and `indexedFeatures` as inputs to our random forest classifier."]},{"metadata":{"colab_type":"code","id":"T1aEpxDPpUEC","colab":{}},"cell_type":"code","source":["# Generate our feature vector.\n","# Note that we're doing the work on the `df` object - we don't create new dataframes, \n","# just add columns to the one we already are using.\n","\n","# the transform method creates the column.\n","\n","df = VectorAssembler(inputCols=double_cols, outputCol=\"features\").transform(df)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"TWG3yfX9pUEH"},"cell_type":"markdown","source":["Let's confirm that the features are there. It's easy to do this in Apache Spark using the `select` and `show` methods on the dataframe.  "]},{"metadata":{"id":"ZlEkUxL9awDD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"d9043838-671f-45b2-970f-e834743ad199","executionInfo":{"status":"ok","timestamp":1549802508142,"user_tz":-180,"elapsed":1763,"user":{"displayName":"Yunus Bulut","photoUrl":"","userId":"10252913168794832935"}}},"cell_type":"code","source":["df.select(\"_c0\", \"features\").show(5)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["+---+--------------------+\n","|_c0|            features|\n","+---+--------------------+\n","|  5|[0.289,-0.0203,-0...|\n","|  5|[0.278,-0.0164,-0...|\n","|  5|[0.28,-0.0195,-0....|\n","|  5|[0.279,-0.0262,-0...|\n","|  5|[0.277,-0.0166,-0...|\n","+---+--------------------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"aoUgW3O2pUEJ"},"cell_type":"markdown","source":["Now we're ready to build the indexers, split our data for training and testing, define our model, and finally chain everything together into a pipeline.\n","\n","__It's important to note that when we execute this cell, we're not actually running our model. At this point, we're only defining its parameters__."]},{"metadata":{"colab_type":"code","id":"m7FKCzVWpUEL","colab":{}},"cell_type":"code","source":["# Build the training indexers / split data / classifier\n","# first we'll generate a labelIndexer\n","labelIndexer = StringIndexer(inputCol=\"_c0\", outputCol=\"indexedLabel\").fit(df)\n","\n","# now generate the indexed feature vector\n","featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df)\n","    \n","# Split the data into training and validation sets (30% held out for testing)\n","(trainingData, testData) = df.randomSplit([TRAINING_DATA_RATIO, 1 - TRAINING_DATA_RATIO])\n","\n","# Train a RandomForest model.\n","rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=RF_NUM_TREES)\n","\n","# Chain indexers and forest in a Pipeline\n","pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"cdM9KpBwpUER"},"cell_type":"markdown","source":["This next cell runs the pipeline, delivering a trained model at the end of the process."]},{"metadata":{"colab_type":"code","id":"rCcHJP5CpUES","scrolled":true,"colab":{}},"cell_type":"code","source":["# Train model.  This also runs the indexers.\n","model = pipeline.fit(trainingData)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"kcFZPyElpUEX"},"cell_type":"markdown","source":["It is now easy to test our model and make predictions simply by using the model's `transform` method on the `testData` dataset."]},{"metadata":{"colab_type":"code","id":"4jsCfvLcpUEY","colab":{}},"cell_type":"code","source":["# Make predictions.\n","predictions = model.transform(testData)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"Hd0VFE7ZpUEd"},"cell_type":"markdown","source":["## Evaluate the model\n","\n","Now we can use the MulticlassClassificationEvaluator to test the model's accuracy."]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1549802586253,"user_tz":-180,"elapsed":6881,"user":{"displayName":"Yunus Bulut","photoUrl":"","userId":"10252913168794832935"}},"id":"M_Gi_iVkpUEf","outputId":"b5984452-686e-47fb-d769-7e11c9cc1fb7","colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["# Select (prediction, true label) and compute test error\n","evaluator = MulticlassClassificationEvaluator(\n","    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n","accuracy = evaluator.evaluate(predictions)\n","\n","print(f\"Test Error = {(1.0 - accuracy):g}\")\n","print(f\"Accuracy = {accuracy:g}\")"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Test Error = 0.0690805\n","Accuracy = 0.930919\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"k6UuHk3PpUEm"},"cell_type":"markdown","source":["## Next Steps\n","\n","We've seen how to prepare data and build a classifier in Spark. You might want to play around with this notebook and learn more about how Spark works. Here are some ideas:\n","\n","- Look at the set of labels, and see if there are any features that would make sense to combine. Spark allows you to map values into a new column.\n","- Identify the most important features among the 561 source features (using PCA or something similar), then reduce the feature set and see if the model performs better.\n","- Modify the settings of the random forest to see if the performance improves.\n","- Use Spark's tools to find other techniques to evaluate the performance of your model. See if you can figure out how to generate an ROC plot, find the AUC value, or plot a confusion matrix."]}]}