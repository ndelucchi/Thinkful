---
type: written content
time: 5 minutes
name: 2.5.4 Solution
author: Bethany Kok

---


´´´

# Assigning large starting error difference number
errdiff=100

# Iteration counter
number_iterations=0

# New stopping threshold
stop_rule=.001

# Provide starting values.
alpha = alpha_start
beta = beta_start

# First run to get the first error value
alpha, beta = step(alpha, beta, learning_rate, x, y)
all_error = [LR_cost_function(alpha, beta, x, y)]

while errdiff > stop_rule:
    
    # Take a step, assigning the results of our step function to feed into
    # the next step.
    alpha, beta = step(alpha, beta, learning_rate, x, y)
    
    # Calculate the error.
    error = LR_cost_function(alpha, beta, x, y)
    
    # Calculate the change in error
    errdiff = all_error[len(all_error)-1]-error
    
    # Store the error to instpect later.
    all_error.append(error)
    
    # Add to iteration count
    number_iterations = number_iterations+1

    
print('\nCoefficients from gradient descent algorithm: \n', beta)
print('\nIntercept from gradient descent algorithm: \n', alpha)
print('\nNumber of iterations: \n', number_iterations)
plt.plot(all_error, 'o', ms=.4)
plt.xlabel('Iteration')
plt.ylabel('Error')
plt.title('Error scores for each iteration')
plt.show()
 ´´´
