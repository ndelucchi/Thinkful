{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have two new regression methods at your fingertips, it's time to give them a spin. In fact, for this challenge, let's put them together! Pick a dataset of your choice with a binary outcome and the potential for at least 15 features. If you're drawing a blank, the crime rates in 2013 dataset has a lot of variables that could be made into a modelable binary outcome.\n",
    "\n",
    "Engineer your features, then create three models. Each model will be run on a training set and a test-set (or multiple test-sets, if you take a folds approach). The models should be:\n",
    "\n",
    "Vanilla logistic regression\n",
    "Ridge logistic regression\n",
    "Lasso logistic regression\n",
    "If you're stuck on how to begin combining your two new modeling skills, here's a hint: the SKlearn LogisticRegression method has a \"penalty\" argument that takes either 'l1' or 'l2' as a value.\n",
    "\n",
    "In your report, evaluate all three models and decide on your best. Be clear about the decisions you made that led to these models (feature selection, regularization parameter selection, model evaluation criteria) and why you think that particular model is the best of the three. Also reflect on the strengths and limitations of regression as a modeling approach. Were there things you couldn't do but you wish you could have done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "distress = pd.read_csv('Financial Distress.csv')\n",
    "x = distress.drop('Financial Distress', axis=1)\n",
    "x['intercept'] = 1 \n",
    "distress['Financial Distress'] = np.where(distress['Financial Distress'] > -0.50, 0, 1)\n",
    "y = distress['Financial Distress']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "endog must be in the unit interval.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-19713425c526>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#initiate our model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/statsmodels/discrete/discrete_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         if (not issubclass(self.__class__, MultinomialModel) and\n\u001b[1;32m    420\u001b[0m                 not np.all((self.endog >= 0) & (self.endog <= 1))):\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"endog must be in the unit interval.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: endog must be in the unit interval."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#initiate our model\n",
    "logit = sm.Logit(x, y)\n",
    "\n",
    "#fit the model\n",
    "logit = logit.fit()\n",
    "\n",
    "# Display results\n",
    "\n",
    "print('R-squared:',round(logit.score(x, y),4))\n",
    "\n",
    "X_train , X_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state=1)\n",
    "trained = logit.fit(X_train, y_train)\n",
    "print('\\nTraining Score:', round(trained.score(X_train, y_train),4))\n",
    "print('Testing Score:', round(trained.score(X_test, y_test),4))\n",
    "\n",
    "y_pred = logit.predict(x)\n",
    "print('\\nMSE:', round(mse(y,y_pred),4))\n",
    "print('rMSE:',round(mse(y,y_pred)**.5,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickdelucchi/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/nickdelucchi/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/nickdelucchi/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.963\n",
      "\n",
      "Training Score: 0.9646\n",
      "Testing Score: 0.9484\n",
      "\n",
      "MSE: 0.037\n",
      "rMSE: 0.1925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickdelucchi/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#initiate our model\n",
    "logreg = LogisticRegression(penalty='l2',C=1e5)\n",
    "\n",
    "#fit the model\n",
    "logreg = logreg.fit(x,y)\n",
    "\n",
    "# Display results\n",
    "print('R-squared:',round(logreg.score(x, y),4))\n",
    "\n",
    "X_train , X_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state=1)\n",
    "trained = logreg.fit(X_train, y_train)\n",
    "print('\\nTraining Score:', round(trained.score(X_train, y_train),4))\n",
    "print('Testing Score:', round(trained.score(X_test, y_test),4))\n",
    "\n",
    "y_pred = logreg.predict(x)\n",
    "print('\\nMSE:', round(mse(y,y_pred),4))\n",
    "print('rMSE:',round(mse(y,y_pred)**.5,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickdelucchi/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/nickdelucchi/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/nickdelucchi/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.9687\n",
      "\n",
      "Training Score: 0.97\n",
      "Testing Score: 0.9429\n",
      "\n",
      "MSE: 0.0327\n",
      "rMSE: 0.1808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickdelucchi/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#initiate our model\n",
    "loglass = LogisticRegression(penalty='l1',C=1e5)\n",
    "\n",
    "#fit the model\n",
    "loglass = loglass.fit(x,y)\n",
    "\n",
    "# Display results\n",
    "print('R-squared:',round(loglass.score(x, y),4))\n",
    "\n",
    "X_train , X_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state=1)\n",
    "trained = loglass.fit(X_train, y_train)\n",
    "print('\\nTraining Score:', round(trained.score(X_train, y_train),4))\n",
    "print('Testing Score:', round(trained.score(X_test, y_test),4))\n",
    "\n",
    "y_pred = loglass.predict(x)\n",
    "print('\\nMSE:', round(mse(y,y_pred),4))\n",
    "print('rMSE:',round(mse(y,y_pred)**.5,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't seem to be any significant difference between these methods for this dataset...what gives?\n",
    "\n",
    "logistic lasso regression seems to edge out the rest of the competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
