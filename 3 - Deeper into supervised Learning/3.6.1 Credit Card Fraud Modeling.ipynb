{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import time\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [datasets](https://www.kaggle.com/mlg-ulb/creditcardfraud) contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...         V21       V22       V23       V24  \\\n",
      "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
      "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
      "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
      "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
      "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
      "\n",
      "        V25       V26       V27       V28  Amount  Class  \n",
      "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "ccf = pd.read_csv('creditcard.csv')\n",
    "print(ccf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.003914\n",
      "         Iterations 13\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  Class   No. Observations:               284807\n",
      "Model:                          Logit   Df Residuals:                   284776\n",
      "Method:                           MLE   Df Model:                           30\n",
      "Date:                Tue, 16 Apr 2019   Pseudo R-squ.:                  0.6922\n",
      "Time:                        07:44:58   Log-Likelihood:                -1114.8\n",
      "converged:                       True   LL-Null:                       -3621.2\n",
      "                                        LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Time       -3.742e-06   2.26e-06     -1.659      0.097   -8.16e-06    6.79e-07\n",
      "V1             0.0960      0.042      2.264      0.024       0.013       0.179\n",
      "V2             0.0094      0.058      0.161      0.872      -0.104       0.123\n",
      "V3            -0.0079      0.053     -0.149      0.881      -0.112       0.096\n",
      "V4             0.6986      0.074      9.454      0.000       0.554       0.843\n",
      "V5             0.1295      0.067      1.944      0.052      -0.001       0.260\n",
      "V6            -0.1198      0.074     -1.626      0.104      -0.264       0.025\n",
      "V7            -0.0969      0.067     -1.453      0.146      -0.228       0.034\n",
      "V8            -0.1739      0.030     -5.711      0.000      -0.234      -0.114\n",
      "V9            -0.2843      0.111     -2.561      0.010      -0.502      -0.067\n",
      "V10           -0.8176      0.097     -8.432      0.000      -1.008      -0.628\n",
      "V11           -0.0621      0.081     -0.762      0.446      -0.222       0.098\n",
      "V12            0.0909      0.087      1.045      0.296      -0.080       0.261\n",
      "V13           -0.3312      0.082     -4.058      0.000      -0.491      -0.171\n",
      "V14           -0.5571      0.062     -8.949      0.000      -0.679      -0.435\n",
      "V15           -0.1141      0.086     -1.330      0.183      -0.282       0.054\n",
      "V16           -0.1908      0.125     -1.526      0.127      -0.436       0.054\n",
      "V17           -0.0216      0.070     -0.309      0.757      -0.159       0.116\n",
      "V18           -0.0131      0.129     -0.102      0.919      -0.266       0.240\n",
      "V19            0.0963      0.097      0.993      0.321      -0.094       0.286\n",
      "V20           -0.4582      0.082     -5.607      0.000      -0.618      -0.298\n",
      "V21            0.3898      0.060      6.494      0.000       0.272       0.507\n",
      "V22            0.6297      0.134      4.707      0.000       0.367       0.892\n",
      "V23           -0.0951      0.058     -1.629      0.103      -0.209       0.019\n",
      "V24            0.1289      0.147      0.874      0.382      -0.160       0.418\n",
      "V25           -0.0761      0.131     -0.582      0.560      -0.332       0.180\n",
      "V26            0.0195      0.190      0.103      0.918      -0.352       0.392\n",
      "V27           -0.8188      0.122     -6.686      0.000      -1.059      -0.579\n",
      "V28           -0.2937      0.088     -3.332      0.001      -0.467      -0.121\n",
      "Amount         0.0009      0.000      2.449      0.014       0.000       0.002\n",
      "intercept     -8.3917      0.249    -33.652      0.000      -8.880      -7.903\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.31 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "# Declare predictors.\n",
    "X_statsmod = ccf.drop('Class', axis=1)\n",
    "\n",
    "# The Statsmodels formulation requires a column with constant value 1 that\n",
    "# will act as the intercept.\n",
    "X_statsmod['intercept'] = 1 \n",
    "\n",
    "# Declare and fit the model.\n",
    "logit = sm.Logit(ccf['Class'], X_statsmod)\n",
    "result = logit.fit()\n",
    "\n",
    "# Lots of information about the model and its coefficients, but the\n",
    "# accuracy rate for predictions is missing.\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy by admission status\n",
      "col_0       0    1\n",
      "Class             \n",
      "0      284273   42\n",
      "1         184  308\n",
      "\n",
      " Percentage accuracy\n",
      "0.9992064801778047\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy. First, get probability that each row will be admitted.\n",
    "pred_statsmod = result.predict(X_statsmod)\n",
    "\n",
    "# Code fraud as 1 if probability is greater than .5.\n",
    "pred_y_statsmod = np.where(pred_statsmod < .5, 0, 1)\n",
    "\n",
    "# Accuracy table.\n",
    "table = pd.crosstab(ccf['Class'], pred_y_statsmod)\n",
    "\n",
    "print('\\n Accuracy by admission status')\n",
    "print(table)\n",
    "print('\\n Percentage accuracy')\n",
    "print((table.iloc[0,0] + table.iloc[1,1]) / (table.sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients\n",
      "[[-7.12203044e-05  3.19003328e-01 -4.84128854e-01 -7.93512062e-01\n",
      "   1.20293469e-01  5.74908256e-02 -5.40509209e-02  3.35311561e-01\n",
      "  -3.74352670e-01 -3.88608714e-01 -2.07048199e-01 -2.86745969e-01\n",
      "   1.86468648e-02 -3.06675484e-01 -6.94620990e-01 -4.27801926e-01\n",
      "  -2.94741919e-01 -4.39987901e-01  3.10692839e-02  2.65185344e-02\n",
      "   9.20026632e-02  2.48888741e-01  3.51032098e-01  6.77179768e-02\n",
      "  -2.44441841e-02 -3.56187711e-01  6.07212397e-02 -8.88577214e-02\n",
      "   2.77997019e-02 -5.58259143e-03]]\n",
      "[-1.62885926]\n",
      "\n",
      " Accuracy by admission status\n",
      "Class       0    1\n",
      "row_0             \n",
      "0      284240  203\n",
      "1          75  289\n",
      "\n",
      " Percentage accuracy\n",
      "0.9990239003957065\n"
     ]
    }
   ],
   "source": [
    "# Declare a logistic regression classifier.\n",
    "# Parameter regularization coefficient C described above.\n",
    "lr = LogisticRegression(C=1e9)\n",
    "y = ccf['Class']\n",
    "X = ccf.drop('Class', axis=1)\n",
    "\n",
    "# Fit the model.\n",
    "fit = lr.fit(X, y)\n",
    "\n",
    "# Display.\n",
    "print('Coefficients')\n",
    "print(fit.coef_)\n",
    "print(fit.intercept_)\n",
    "pred_y_sklearn = lr.predict(X)\n",
    "\n",
    "print('\\n Accuracy by admission status')\n",
    "print(pd.crosstab(pred_y_sklearn, y))\n",
    "\n",
    "print('\\n Percentage accuracy')\n",
    "print(lr.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.998982\n",
      "\n",
      "Training Score:  0.999254\n",
      "Testing Score:  0.999122\n",
      "\n",
      "MSE:  0.000772\n",
      "rMSE:  0.027793\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print('Accuracy: ',round(lr.score(X, y),6))\n",
    "\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)\n",
    "trained = lr.fit(X_train, y_train)\n",
    "print('\\nTraining Score: ', round(trained.score(X_train, y_train),6))\n",
    "print('Testing Score: ', round(trained.score(X_test, y_test),6))\n",
    "\n",
    "#cvs = cross_val_score(lr, X, y, cv=10)\n",
    "#print('\\nCross Validation Score: {} +/- {}'.format(round(cvs.mean(),6), round(cvs.std(),6)))\n",
    "\n",
    "y_pred = lr.predict(X)\n",
    "print('\\nMSE: ', round(mse(y,y_pred),6))\n",
    "print('rMSE: ',round(mse(y,y_pred)**.5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I(false positive) errors: 0.00021944743136781584\n",
      "Percent Type II(false negative) errors: 0.000526673835282758\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I(false positive) errors: 0.00026333345037042236\n",
      "Percent Type II(false negative) errors: 0.0006144447175309856\n"
     ]
    }
   ],
   "source": [
    "predict_train = lr.predict(X_train)\n",
    "predict_test = lr.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I(false positive) errors: {}\\n'\n",
    "    'Percent Type II(false negative) errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I(false positive) errors: {}\\n'\n",
    "    'Percent Type II(false negative) errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score: 0.997651 +/- 0.004507\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(lr, X, y, cv=10)\n",
    "print('Cross Validation Score: {} +/- {}'.format(round(cvs.mean(),6), round(cvs.std(),6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 800.7953999042511 seconds ---\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 1.7555794509425268e-05\n",
      "Percent Type II errors: 0.00027211481489609164\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.00010533338014816895\n",
      "Percent Type II errors: 0.00040377795723464763\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# We'll make 3000 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 4,\n",
    "          'subsample' : 0.9,\n",
    "          'loss': 'exponential',\n",
    "          'max_leaf_nodes': 6}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
