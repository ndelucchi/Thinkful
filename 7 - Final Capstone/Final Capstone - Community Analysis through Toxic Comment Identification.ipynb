{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsJeNVJp_9fJ"
   },
   "source": [
    "# Community Analysis through Toxic Comment Identification\n",
    "\n",
    "In this project we aim to:\n",
    "* Identify toxic comments with high accuracy\n",
    "* Explore ensemble and tensorflow models\n",
    "* Prepare the final model for production use  \n",
    "* Have fun!\n",
    "\n",
    "The data that we'll be training with was provided by YouTube from [a Kaggle competition they ran in 2018](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data).\n",
    "\n",
    "We'll be performing Term Frequency-Inverse Document Frequency analysis on the comments to train our models with. This will represent the comments in a numerical fashion that can be easily and meaningfully understood by the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tW_ZFKtqRSM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import neighbors, ensemble, tree,preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit, RandomizedSearchCV\n",
    "\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "_IWa6ocl8fHB",
    "outputId": "f4b49d66-e553-46bc-e7a6-020e0a35931e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  ... identity_hate\n",
       "0  0000997932d777bf  ...             0\n",
       "1  000103f0d9cfb60f  ...             0\n",
       "2  000113f07ec002fd  ...             0\n",
       "3  0001b41b1c6bb37e  ...             0\n",
       "4  0001d958c54c6e35  ...             0\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import test.csv\n",
    "toxic = pd.read_csv('train.csv')\n",
    "\n",
    "toxic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DcLWFvqy1iJ"
   },
   "source": [
    "Looks like there's a few different varieties of toxicity for these comments, where 1 indicates toxic. Let's focus on a boolean of toxic or not, disreguarding subtype. For this we'll need to create a few features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "S7IVj6bjyv3Y",
    "outputId": "914fc108-d06e-43b1-bd76-b93a35fa9486"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>toxic_total</th>\n",
       "      <th>toxic_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  ... toxic_bool\n",
       "0  0000997932d777bf  ...          0\n",
       "1  000103f0d9cfb60f  ...          0\n",
       "2  000113f07ec002fd  ...          0\n",
       "3  0001b41b1c6bb37e  ...          0\n",
       "4  0001d958c54c6e35  ...          0\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature engineering\n",
    "#total_toxic which is the sum of all toxic subtypes\n",
    "#a comment can have a max value of 6 if it were all tyoes of toxic\n",
    "toxic['toxic_total'] = (toxic['toxic'] + toxic['severe_toxic'] +\n",
    "                        toxic['obscene'] + toxic['threat'] + \n",
    "                        toxic['insult'] + toxic['identity_hate'])\n",
    "\n",
    "#we're interested in a boolean measure of toxicity so any value over 0 becomes 1\n",
    "toxic['toxic_bool'] = np.where(toxic['toxic_total']>0,1,0)\n",
    "\n",
    "toxic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "kO7M-qamyqZQ",
    "outputId": "46575b04-22b0-450a-aad4-22d5fc5b14a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic_bool, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic['toxic_bool'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s5DJkOVr5jqU"
   },
   "source": [
    "So we see ~10% representation of toxic comments, that's a pretty harsh disparity and we'll address that inequality later to better feed our model. If we left it as is then the model could just predict \"non-toxic\" for all comments and be correct about 90% of the time.\n",
    "\n",
    "\n",
    "Let's take a look at some statistics for comments that we'll be training with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "kFGxeOCJHJRO",
    "outputId": "e7272a00-0b5b-41d7-c5a3-3af9530f306a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Comment Length: 394\n",
      "Standard Deviation: 591\n",
      "Max Comment Length: 5000\n"
     ]
    }
   ],
   "source": [
    "lens = toxic.comment_text.str.len()\n",
    "print(f'Average Comment Length: {round(lens.mean())}\\nStandard Deviation: {round(lens.std())}\\nMax Comment Length: {round(lens.max())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-AH0n6cO67Ri"
   },
   "source": [
    "Great! Now let's take a look at the distribution of word count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "XgXChHoYHOsz",
    "outputId": "5cf6ffe6-88a4-4b1f-8022-0991e3af2388"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7efc594057f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nboaGJKmboSFJ6mZoSJK6GRqSpG7/Hw6s8kXxJy56AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qX3kHMLm7fv3"
   },
   "source": [
    "Alright, we see a strong right skewed distribution, as we might expect, but at least one person wrote a wopping 5000 word comment!\n",
    "\n",
    "Let's being to prepare out model for TF-IDF analysis! We'll start with a train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADJLp14G2V8n"
   },
   "outputs": [],
   "source": [
    "# Define the features and the outcome.\n",
    "X = toxic['comment_text']\n",
    "y = toxic['toxic_bool']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PrqUFhQK8C4P"
   },
   "source": [
    "Next we'll vectorize, reshape, perform TF-IDF,  and normalize  X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "IRa6k7npq5iS",
    "outputId": "b37f513b-b98a-46fd-b7cc-b78c32229c5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer complete\n",
      "tocsr complete\n",
      "tf-idf complete\n",
      "normalization complete\n",
      "\n",
      " -- 52.8681 seconds for results--\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=3, # only use words that appear at least three times\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case\n",
    "                             use_idf=True,#we want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "#Applying the vectorizer\n",
    "toxic_tfidf_train = vectorizer.fit_transform(X_train)\n",
    "toxic_tfidf_test = vectorizer.transform(X_test)\n",
    "print(\"vectorizer complete\")\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = toxic_tfidf_train.tocsr()\n",
    "X_test_tfidf_csr = toxic_tfidf_test.tocsr()\n",
    "print('tocsr complete')\n",
    "\n",
    "#number of sentences\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per sentence\n",
    "tfidf_bysent = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each sentence, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bysent[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "print('tf-idf complete')\n",
    "    \n",
    "# Normalize the data.\n",
    "X_train_norm = normalize(X_train_tfidf_csr)\n",
    "X_test_norm = normalize(X_test_tfidf_csr)\n",
    "print('normalization complete')\n",
    "\n",
    "t= round((time.time() - start_time),4)\n",
    "print(\"\\n -- %s seconds for results--\" % t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nFPCe-0yTx5J",
    "outputId": "9613750f-215d-4632-b80a-d759fb048aa2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119678,)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5xkiD5QBeUR"
   },
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer,open('tf_idf_vectorize.sav','wb'))\n",
    "vectorizer = pickle.load(open('tf_idf_vectorize.sav','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NsjWqs094WLS"
   },
   "outputs": [],
   "source": [
    "#X_train_vec = vectorizer.transform(X_train)\n",
    "#X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VnlLHvkz8atB"
   },
   "source": [
    "Next we'll address the class imbalance that we noticed earlier. by the end of it, we'll have equal counts non-toxic and toxic to train on. This will provide a better base for training and reduce bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_rI3NV5e9MZJ",
    "outputId": "aec86791-e510-41ec-aba1-4a0ec58baaaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- 1100.5114 seconds for results--\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "smt = SMOTETomek(random_state=42)\n",
    "#class balancing\n",
    "#only perform on the training set, this reduces bias in fitting the model.\n",
    "X_res, y_res = smt.fit_resample(X_train_norm, y_train)\n",
    "\n",
    "t= round((time.time() - start_time),4)\n",
    "print(\"\\n -- %s seconds for results--\" % t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6KZfsSZ5HVyR"
   },
   "outputs": [],
   "source": [
    "#save transformed and class-balanced training set\n",
    "# file1 = open('X_train_final.pickle','wb')\n",
    "# pickle.dump(X_res,file1)\n",
    "# file1.close()\n",
    "# file2 = open('y_train_pre.pickle','wb')\n",
    "# pickle.dump(y_res,file2)\n",
    "# file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XjX4N4dsFXa"
   },
   "outputs": [],
   "source": [
    "file3 = open('X_train_final.pickle','rb')\n",
    "X_train_final = pickle.load(file3)\n",
    "file3.close()\n",
    "\n",
    "file4 = open('y_train_pre.pickle','rb')\n",
    "y_train_pre = pickle.load(file4)\n",
    "file4.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "S3_FSSwL84U7",
    "outputId": "e9e5a52d-49be-413e-b94c-4e6d219424d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 214990\n",
      "Measure of distribution of X: 0.5\n"
     ]
    }
   ],
   "source": [
    "#confirm equal distribution\n",
    "print(f'Length of X: {len(y_train_pre)}')\n",
    "print(f'Measure of distribution of X: {y_train_pre.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7mW_NSfK9hI5",
    "outputId": "643bf801-b375-4850-8505-22905d5414b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214990, 44454)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_4FvRXEOXOBf"
   },
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=3, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "WR-TRUkoqx7E",
    "outputId": "e4d5e19e-6202-4cea-a038-72bfc4524d2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC: 0.951211 (0.002019)\n",
      "KNN: 0.905597 (0.002282)\n",
      "DTC: 0.941353 (0.003790)\n",
      "GBC: 0.939683 (0.001764)\n",
      "ABC: 0.947368 (0.002865)\n",
      "ETC: 0.952381 (0.002813)\n",
      "\n",
      " -- 1319.1425 seconds for results--\n"
     ]
    }
   ],
   "source": [
    "# Spot Check Algorithms\n",
    "start_time = time.time()\n",
    "models = []\n",
    "models.append(('RFC', ensemble.RandomForestClassifier()))\n",
    "models.append(('KNN', neighbors.KNeighborsClassifier()))\n",
    "models.append(('DTC', tree.DecisionTreeClassifier()))\n",
    "models.append(('GBC', ensemble.GradientBoostingClassifier()))\n",
    "models.append(('ABC', ensemble.AdaBoostClassifier()))\n",
    "models.append(('ETC', ensemble.ExtraTreesClassifier()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    split = StratifiedShuffleSplit(n_splits=3, random_state=1337)\n",
    "    model = model.fit(X_train_final,y_train_pre)\n",
    "    cv_results = cross_val_score(model, X_test_norm, y_test, cv=split, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "t= round((time.time() - start_time),4)\n",
    "print(\"\\n -- %s seconds for results--\" % t) #22min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w5DCr2dvhbb0"
   },
   "source": [
    "Wow, great! each of the models performed quite well but ExtraTreesClassifier just barely edges out RandomForestClassifier as the top performer without any hyper praramter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "pUQNB1WMYQ1S",
    "outputId": "13aa0c11-a831-43a4-f5a2-726ce922f42f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETC test score: 0.9543024227234754, STD: 0.00031258624785076005\n",
      "\n",
      " -- 255.4806 seconds for results--\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "ETC = ensemble.ExtraTreesClassifier()\n",
    "ETC.fit(X_train_final,y_train_pre)\n",
    "cv_results = cross_val_score(ETC, \n",
    "                             X_test_norm, \n",
    "                             y_test, \n",
    "                             cv=split,\n",
    "                             scoring='accuracy')\n",
    "\n",
    "\n",
    "print(f'ETC test score: {cv_results.mean()}, STD: {cv_results.std()}')\n",
    "\n",
    "t= round((time.time() - start_time),4)\n",
    "print(\"\\n -- %s seconds for results--\" % t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3krj0BW8nyp"
   },
   "outputs": [],
   "source": [
    "#pickle the ExtraTreesClassifier to get it production ready\n",
    "file = open('toxic_ETC.sav','wb')\n",
    "pickle.dump(ETC,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VL8BreFn74IB"
   },
   "source": [
    "Great, we have a well-performing model and we've saved it for production use. Let's compare it's performance to a Tensorflow/Keras model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rBr06qCrUtek",
    "outputId": "26d0a956-36fa-408f-85fc-530f826c7969"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_categorical complete\n"
     ]
    }
   ],
   "source": [
    "# Convert y to categorical to make compatible with keras model\n",
    "y_train_final = to_categorical(y_train_pre)\n",
    "y_test_final = to_categorical(y_test)\n",
    "print('to_categorical complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-v8xS36S1VW"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# Import various componenets for model building\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "FOnQLOJpOo2Y",
    "outputId": "cba11365-29cb-4784-e4f7-0682a77ec091"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0723 01:53:43.638366 140127750592384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0723 01:53:43.687673 140127750592384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0723 01:53:43.696128 140127750592384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               4445500   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 4,465,902\n",
      "Trainable params: 4,465,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize the constructor\n",
    "model = Sequential()\n",
    "\n",
    "# Add an input layer \n",
    "model.add(Dense(100, activation='relu', input_dim= X_train_final.shape[1] ))\n",
    "\n",
    "# Add a hidden layer \n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "# Add a hidden layer \n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "# Add an output layer \n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "qw9Hqq7sTAuW",
    "outputId": "3676bd13-d8d5-4c22-ef99-c91d92529bd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 214990 samples, validate on 39893 samples\n",
      "Epoch 1/3\n",
      "214990/214990 [==============================] - 65s 301us/step - loss: 0.1513 - acc: 0.9506 - val_loss: 0.2036 - val_acc: 0.9492\n",
      "Epoch 2/3\n",
      "214990/214990 [==============================] - 64s 298us/step - loss: 0.0192 - acc: 0.9946 - val_loss: 0.2544 - val_acc: 0.9501\n",
      "Epoch 3/3\n",
      "214990/214990 [==============================] - 64s 297us/step - loss: 0.0079 - acc: 0.9980 - val_loss: 0.2610 - val_acc: 0.9500\n",
      "Test loss: 0.2609545102933453\n",
      "Test accuracy: 0.9500413606456877\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "                   \n",
    "model.fit(X_train_final, y_train_final, epochs=3, batch_size=512, verbose=1, validation_data=(X_test_norm, y_test_final))\n",
    "score = model.evaluate(X_test_norm, y_test_final, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GbRH7wDyjkrP"
   },
   "source": [
    "Great, using 'relu' activation yielded some pretty great results. I tested out numerous combinations of other activation functions but none yielded better results, measuring 0.93 and lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOkgVk8HTAi3"
   },
   "outputs": [],
   "source": [
    "file = open('tensorflow.sav','wb')\n",
    "pickle.dump(model,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eCVn0xuy8U5d"
   },
   "source": [
    "Now we pickle the trained model to use to predict whether or not a comment is toxic. Then we'll average the result for each comment across the videos of a channel to get an overall representation of the toxicity of the community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Mfip8WmJf7U"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The ensemble method ExtraTreesClassifier and a Mean-Squared Error Tensorflow/Keras model performed quite well but ultimately the ensemble model ExtraTreesClassifier just barely performed the better so we'll go ahead and use a pickled version of that model going forward to analyze the youtube comments that we'll collect. \n",
    "\n",
    "## Obstacles\n",
    "I thought that my biggest problem in this project would be the size of the data but it actually was determining the appropriate order of each step to get it to. I'm sure it seems obvious now in hindsight.\n",
    "\n",
    "Another obstacle we encountered was significantly poorer performance of th model before class balancing was performed on the toxic boolean feature, where the minority represented only about 10% of the total inputs.\n",
    "\n",
    "## The Future\n",
    "One of the drawbacks with working with something that has such varied user input is that eventually the corpus will be a little out of date, missing relevant terms (i.e. slag, etc) that will come into use. Of course, a large amount of the provided corpus will remain relevant so overall model performance shouldn't dip too significantly. But when the day comes to re-fit the model, it could be quite taxing to label a brand new set of a few hundred thousand comments as various kinds of toxic.\n",
    "\n",
    "## Applying the model to a non-training dataset\n",
    "Please select the file titled \"Application of Toxic-Community Model\" to see the applicaiton of the model generated in this project"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final Capstone - Community Analysis through Toxic Comment Identification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
