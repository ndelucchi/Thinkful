{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of Toxic-Community Model\n",
    "\n",
    "Here we are applying the trained toxic comment identification model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T03:18:21.740951Z",
     "start_time": "2019-07-24T03:18:18.645989Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from selenium.webdriver import Chrome\n",
    "from contextlib import closing\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Comments\n",
    "Now we'll be getting the comments from the community we'd like to analyze. As a test I've selected a channel that is proud of it's community and believes it to be largely non-toxic. This community is [KindaFunny](https://www.youtube.com/channel/UCb4G6Wao_DeFr1dm8-a9zjg).\n",
    "\n",
    "We'll be accessing the Youtube API to get a list of the 50 most recent video IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T03:24:53.002727Z",
     "start_time": "2019-07-24T03:24:50.961507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have video ids for 50 videos\n"
     ]
    }
   ],
   "source": [
    "API_KEY = #put your API key here\n",
    "\n",
    "channel_ids= ['UCb4G6Wao_DeFr1dm8-a9zjg'] #input at some point\n",
    "\n",
    "for channel in channel_ids:\n",
    "    channel_url = f'https://www.googleapis.com/youtube/v3/channels?part=contentDetails&id={channel}&key={API_KEY}'\n",
    "channel_videos = requests.get(channel_url).json()\n",
    "\n",
    "#pull upload playlist ID\n",
    "playlist_id = channel_videos['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "\n",
    "#generate playlist url\n",
    "playlist_url = f'https://www.googleapis.com/youtube/v3/playlistItems?part=snippet&maxResults=50&playlistId={playlist_id}&key={API_KEY}'\n",
    "\n",
    "#pulling video ids from uploads playlist\n",
    "video_ids = requests.get(playlist_url).json()\n",
    "\n",
    "#.json of video IDs\n",
    "video_ids\n",
    "\n",
    "#print video IDs from .json\n",
    "video_id_values = []\n",
    "for i in video_ids['items']:\n",
    "    try:\n",
    "        video_id_values.append(i['snippet']['resourceId']['videoId'])\n",
    "    except:\n",
    "        print('No video ids')\n",
    "        pass\n",
    "print(f'We have video ids for {len(video_id_values)} videos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will iterate through this list to pull comments from each of these 50 videos. This takes some time and if this process were ever productionized for the public then some adjustments would have to be made as I don't think the customer would be interested in waiting about 30 minutes for results. If it were productionized provately then it could be adjusted to get even more comments across these videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T04:16:07.274459Z",
     "start_time": "2019-07-24T03:53:15.701799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n",
      "-1371.5600349903107 secs\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "newest_comments = []\n",
    "\n",
    "for v in video_id_values:\n",
    "    options = Options()\n",
    "    #options.headless = True\n",
    "    driver = webdriver.Chrome(chrome_options=options)\n",
    "    driver.maximize_window()\n",
    "    try:\n",
    "        driver.get(f'https://www.youtube.com/watch?v={v}')\n",
    "        wait = WebDriverWait(driver,20)\n",
    "        time.sleep(1)\n",
    "        driver.find_element_by_tag_name('body').send_keys('m')\n",
    "        #down arrows to move down page\n",
    "        wait.until(EC.visibility_of_element_located((By.TAG_NAME, \"body\"))).send_keys(Keys.DOWN)\n",
    "        for i in range(15):\n",
    "            driver.find_element_by_tag_name('body').send_keys(Keys.DOWN)    \n",
    "        #select and click 'sort by' drop down menu\n",
    "        time.sleep(1)\n",
    "        wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"trigger\"]'))).click()\n",
    "        time.sleep(1)\n",
    "        #select 'Newest Comments' from drop down menu\n",
    "        wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"menu\"]/a[2]/paper-item/paper-item-body/div[1]'))).click()\n",
    "\n",
    "        for item in range(8): #by increasing the highest range you can get more content \n",
    "                #scroll down the page further\n",
    "                driver.find_element_by_tag_name('body').send_keys(Keys.END)                            \n",
    "                time.sleep(1) #higher means more time to load the page as we scroll\n",
    "\n",
    "        for comment in wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"#comment #content-text\"))):\n",
    "                newest_comments.append(comment.text)\n",
    "                #print(comment.text)\n",
    "        driver.quit()\n",
    "        \n",
    "    except:\n",
    "        driver.quit()\n",
    "        pass\n",
    "\n",
    "print (\"done!\")\n",
    "print(start_time-time.time(), 'secs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T04:17:19.161151Z",
     "start_time": "2019-07-24T04:17:19.143616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3577"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the number of comments we gathered\n",
    "len(newest_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T04:39:50.755167Z",
     "start_time": "2019-07-24T04:39:50.669284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            comments\n",
      "0  Videogames seem to catchup to high end cgi so ...\n",
      "1                      THE CUT TO WHEN GREG SAID CAT\n",
      "2  Yea this movie sucks everyone except Andy are ...\n",
      "3  Clapping at the theater shouldnt be used as a ...\n",
      "4  Andys wheeze laugh is one of my favorite thing...\n"
     ]
    }
   ],
   "source": [
    "#load into dataframe\n",
    "input_comments = pd.DataFrame(newest_comments)\n",
    "#name comments column\n",
    "input_comments.columns = ['comments']\n",
    "#clean odd characters\n",
    "weird = [':','%',';',',','.','!','\\\"','\\'','*','&','#','@','$','+','=','-','_']\n",
    "for item in weird:\n",
    "    input_comments['comments'] = input_comments['comments'].apply(lambda x : x.replace(item,''))\n",
    "\n",
    "#input_comments.to_csv('kf_comments.csv')\n",
    "print(input_comments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T04:28:03.935926Z",
     "start_time": "2019-07-24T04:28:03.919918Z"
    }
   },
   "outputs": [],
   "source": [
    "#load saved .csv to save time later\n",
    "input_com = pd.read_csv('kf_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T04:34:01.443999Z",
     "start_time": "2019-07-24T04:34:01.248801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "#import vectorizer\n",
    "file = open('tf_idf_vectorize.sav','rb')\n",
    "vectorizer = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "#Applying the vectorizer\n",
    "toxic_tfidf=vectorizer.transform(input_com)\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_tfidf_csr = toxic_tfidf.tocsr()\n",
    "\n",
    "#number of sentences\n",
    "n = X_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per sentence\n",
    "tfidf_bysent = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each sentence, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_tfidf_csr.nonzero()):\n",
    "    tfidf_bysent[i][terms[j]] = X_tfidf_csr[i, j]\n",
    "\n",
    "# Normalize the data.\n",
    "input_X_norm = normalize(X_tfidf_csr)\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T04:32:07.212655Z",
     "start_time": "2019-07-24T04:32:07.090869Z"
    }
   },
   "outputs": [],
   "source": [
    "#load the pickled fit-model\n",
    "filename = 'toxic_ETC.sav'\n",
    "file = open(filename, 'rb')\n",
    "model = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T04:36:46.281389Z",
     "start_time": "2019-07-24T04:36:46.265618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of this community being toxic is 18.69%\n"
     ]
    }
   ],
   "source": [
    "#Next we take the mean of the probability of each comment being toxic\n",
    "#[1] is the probaility of being a toxic comment, [0] is the probability for not toxic\n",
    "new_data_result = model.predict_proba(input_X_norm).mean(axis=0)[1]\n",
    "\n",
    "#For [1] value closer to 0 is less toxic, for [0] value closer to 1 is less toxic\n",
    "print(f\"The probability of this community being toxic is {round(new_data_result*100,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is congruent with what the community anticipated, claiming that their community is largely non-toxic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider checking out these other channels:\n",
    "* FoxNews = \"UCXIJgqnII2ZOINSWNOGFThA\"\n",
    "* JoeRogan = 'UCzQUP1qoWDoEbmsQxvdjxgQ' \n",
    "* PewDiePie = 'UC-lHJZR3Gqxm24_Vd_AJ5Yw' - \n",
    "* CriticalRole = 'UCpXBGqwsBkpvcYjsJBQ7LEQ' \n",
    "* PhillipDeFranco = \"UClFSU9_bUb4Rc6OYfTt5SPw\" \n",
    "* RetroReplay = \"UCx42AsHSKnjEfPXk0YiGCyw\"\n",
    "#### or pick your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
