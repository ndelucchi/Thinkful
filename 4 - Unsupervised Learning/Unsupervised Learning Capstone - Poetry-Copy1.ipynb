{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project you'll dig into a large amount of text and apply most of what you've covered in this unit and in the course so far.\n",
    "\n",
    "First, pick a set of texts. This can be either a series of novels, chapters, or articles. Anything you'd like. It just has to have multiple entries of varying characteristics. At least 100 should be good. There should also be at least 10 different authors, but try to keep the texts related (either all on the same topic or from the same branch of literature - something to make classification a bit more difficult than obviously different subjects).\n",
    "\n",
    "This capstone can be an extension of your NLP challenge if you wish to use the same corpus. If you found problems with that data set that limited your analysis, however, it may be worth using what you learned to choose a new corpus. Reserve 25% of your corpus as a test set.\n",
    "\n",
    "1. The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?\n",
    "\n",
    "2. Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. \n",
    "\n",
    "3. Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance.\n",
    "\n",
    "4. Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is it's performance consistent? If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn import ensemble\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import neighbors\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit\n",
    "\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean the data.\n",
    "leaves = gutenberg.paras('whitman-leaves.txt')\n",
    "leaves_paras=[]\n",
    "for paragraph in leaves:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    leaves_paras.append(' '.join(para))\n",
    "\n",
    "paradise = gutenberg.paras('milton-paradise.txt')\n",
    "paradise_paras=[]\n",
    "for paragraph in paradise:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    paradise_paras.append(' '.join(para))\n",
    "    \n",
    "    \n",
    "blake = gutenberg.paras('blake-poems.txt')\n",
    "blake_paras=[]\n",
    "for paragraph in blake:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    blake_paras.append(' '.join(para))\n",
    "\n",
    "    \n",
    "new_corpus = PlaintextCorpusReader(\"\",'.txt')    \n",
    "    \n",
    "hiawatha = new_corpus.paras(\"The Song Of Hiawatha, by Henry W. Longfellow.txt\")\n",
    "hiawatha_paras=[]\n",
    "for paragraph in hiawatha:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    hiawatha_paras.append(' '.join(para))\n",
    "\n",
    "\n",
    "endymion = new_corpus.paras(\"Endymion, by John Keats.txt\")\n",
    "endymion_paras=[]\n",
    "for paragraph in endymion:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    endymion_paras.append(' '.join(para))\n",
    "\n",
    "\n",
    "odyssey = new_corpus.paras(\"The Odyssey by Homer.txt\")\n",
    "odyssey_paras=[]\n",
    "for paragraph in odyssey:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    odyssey_paras.append(' '.join(para))\n",
    "\n",
    "\n",
    "burns = new_corpus.paras(\"The Complete Works of Robert Burns.txt\")\n",
    "burns_paras=[]\n",
    "for paragraph in burns:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    burns_paras.append(' '.join(para))\n",
    "\n",
    "\n",
    "sea = new_corpus.paras(\"Sea Garden by Hilda Doolittle.txt\")\n",
    "sea_paras=[]\n",
    "for paragraph in sea:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    sea_paras.append(' '.join(para))\n",
    "    \n",
    "\n",
    "beowulf = new_corpus.paras(\"Beowulf by Leslie Hall.txt\")\n",
    "beowulf_paras=[]\n",
    "for paragraph in beowulf:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    beowulf_paras.append(' '.join(para))\n",
    "\n",
    "\n",
    "sappho = new_corpus.paras(\"Sappho- One Hundred Lyrics by Bliss Carman.txt\")\n",
    "sappho_paras=[]\n",
    "for paragraph in sappho:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    sappho_paras.append(' '.join(para))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ Paradise Lost by John Milton 1667 ]</td>\n",
       "      <td>Milton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Book I</td>\n",
       "      <td>Milton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Of Man ' s first disobedience , and the fruit ...</td>\n",
       "      <td>Milton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Book II</td>\n",
       "      <td>Milton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>High on a throne of royal state , which far Ou...</td>\n",
       "      <td>Milton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1\n",
       "0              [ Paradise Lost by John Milton 1667 ]  Milton\n",
       "1                                             Book I  Milton\n",
       "2  Of Man ' s first disobedience , and the fruit ...  Milton\n",
       "3                                            Book II  Milton\n",
       "4  High on a throne of royal state , which far Ou...  Milton"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into paragraphs.\n",
    "paradise_paras = [[para, \"Milton\"] for para in paradise_paras]\n",
    "blake_paras = [[para, \"Blake\"] for para in blake_paras]\n",
    "leaves_paras = [[para, \"Whitman\"] for para in leaves_paras]\n",
    "hiawatha_paras = [[para, \"Longfellow\"] for para in hiawatha_paras]\n",
    "endymion_paras = [[para, \"Keats\"] for para in endymion_paras]\n",
    "odyssey_paras = [[para, \"Homer\"] for para in odyssey_paras]\n",
    "burns_paras = [[para, \"Burns\"] for para in burns_paras]\n",
    "sea_paras = [[para, \"Doolittle\"] for para in sea_paras]\n",
    "beowulf_paras = [[para, \"Beowulf\"] for para in beowulf_paras]\n",
    "sappho_paras = [[para, \"Sappho\"] for para in sappho_paras]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "paragraphs = pd.DataFrame(paradise_paras + blake_paras + \n",
    "                         leaves_paras + hiawatha_paras + \n",
    "                         endymion_paras + odyssey_paras +\n",
    "                         burns_paras + sea_paras +\n",
    "                         beowulf_paras + sappho_paras)\n",
    "paragraphs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "284\n",
      "2478\n",
      "681\n",
      "140\n",
      "216\n",
      "11621\n",
      "252\n",
      "1164\n",
      "461\n",
      "17326\n"
     ]
    }
   ],
   "source": [
    "print(len(paradise_paras))\n",
    "print(len(blake_paras))\n",
    "print(len(leaves_paras))\n",
    "print(len(hiawatha_paras))\n",
    "print(len(endymion_paras))\n",
    "print(len(odyssey_paras))\n",
    "print(len(burns_paras))\n",
    "print(len(sea_paras))\n",
    "print(len(beowulf_paras))\n",
    "print(len(sappho_paras))\n",
    "print(len(paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(paragraphs.drop(1,axis=1),paragraphs[1], \n",
    "                                                    test_size=0.25, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=3, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 9301\n"
     ]
    }
   ],
   "source": [
    "#Applying the vectorizer\n",
    "paras_tfidf=vectorizer.fit_transform(paragraphs.drop(1,axis=1)[0].tolist())\n",
    "print(\"Number of features: %d\" % paras_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(paras_tfidf, test_size=0.25, random_state=0)\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "X_test_tfidf_csr = X_test_tfidf.tocsr()\n",
    "\n",
    "#number of sentences\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per sentence\n",
    "tfidf_bysent = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each sentence, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bysent[i][terms[j]] = X_train_tfidf_csr[i, j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    [ Paradise Lost by John Milton 1667 ]\n",
       "1                                                   Book I\n",
       "2        Of Man ' s first disobedience , and the fruit ...\n",
       "3                                                  Book II\n",
       "4        High on a throne of royal state , which far Ou...\n",
       "5                                                 Book III\n",
       "6        Hail , holy Light , offspring of Heaven firstb...\n",
       "7        00021053 Thou , therefore , whom thou only can...\n",
       "8                                                  Book IV\n",
       "9        O , for that warning voice , which he , who sa...\n",
       "10       00081429 Which to our general sire gave prospe...\n",
       "11                                                  Book V\n",
       "12       Now Morn , her rosy steps in the eastern clime...\n",
       "13                                                 Book VI\n",
       "14       All night the dreadless Angel , unpursued , Th...\n",
       "15                                                Book VII\n",
       "16       Descend from Heaven , Urania , by that name If...\n",
       "17                                               Book VIII\n",
       "18       The Angel ended , and in Adam ' s ear So charm...\n",
       "19                                                 Book IX\n",
       "20       No more of talk where God or Angel guest With ...\n",
       "21       00482129 If answerable style I can obtain Of m...\n",
       "22                                                  Book X\n",
       "23       Mean while the heinous and despiteful act Of S...\n",
       "24                                                 Book XI\n",
       "25       Undoubtedly he will relent , and turn From his...\n",
       "26                                                Book XII\n",
       "27       As one who in his journey bates at noon , Thou...\n",
       "28                                           [ The End ]\u001a\u001a\n",
       "29                         [ Poems by William Blake 1789 ]\n",
       "                               ...                        \n",
       "17296    Before thy least lost murmur I must sigh , 5 O...\n",
       "17297                                                 XCIX\n",
       "17298    Over the wheat - field , Over the hill - crest...\n",
       "17299    What premonition , O purple swallow , 10 Told ...\n",
       "17300    Soon will a shepherd In rugged Dacia , Folding...\n",
       "17301    This very hour 25 In Mitylene , Will not a you...\n",
       "17302                                                    C\n",
       "17303    Once more the rain on the mountain , Once more...\n",
       "17304                        Warm is the sun in the city .\n",
       "17305    Gentlier now falls the twilight , With the sli...\n",
       "17306    Gladlier now crimson morning Flushes fair - bu...\n",
       "17307    Ah , but what burden of sorrow Tinges their sl...\n",
       "17308    Shall they then never behold thee , Nevermore ...\n",
       "17309    Nevermore answer thy glowing Youth with their ...\n",
       "17310    Heedless , assuaged , art thou sleeping Where ...\n",
       "17311    Hast thou no passion nor pity For thy deserted...\n",
       "17312     Nay , but in vain their clear voices Call thee .\n",
       "17313    In the faint fragrance of flowers , On the swe...\n",
       "17314                                             EPILOGUE\n",
       "17315    Now the hundred songs are made , And the pause...\n",
       "17316    On a day the frost will come , 5 Walking throu...\n",
       "17317                      On a day ( Oh , far from now !)\n",
       "17318    All the happy songs he wrought From remembranc...\n",
       "17319    Frail as dew upon the grass Or the spindrift o...\n",
       "17320    Nay , but something of thy love , Passion , te...\n",
       "17321    Must imperishably cling 25 To the cadence of t...\n",
       "17322    Wild and fleeting as the notes Blown upon a wo...\n",
       "17323    For the transport in their rhythm Was the thro...\n",
       "17324    When the golden days arrive , With the swallow...\n",
       "17325    Long hereafter shall thy name Be recalled thro...\n",
       "Name: 0, Length: 17326, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs.drop(1,axis=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9696783130675697\n",
      "\n",
      "Test set score: 0.8047091412742382\n",
      "\n",
      "Cross Validation:\n",
      "    0.76 (+/- 0.01)\n",
      "[0.75345622 0.73502304 0.76267281 0.78341014 0.7718894  0.76958525\n",
      " 0.75576037 0.74884793 0.76728111 0.74884793]\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "rfc.fit(X_train_tfidf_csr, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train_tfidf_csr, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test_tfidf_csr, y_test))\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=10, random_state=1337)\n",
    "\n",
    "score = cross_val_score(rfc, X_test_tfidf_csr, y_test, cv=split, scoring='accuracy')\n",
    "print(\"\\nCross Validation:\\n    %0.2f (+/- %0.2f)\" % (score.mean(), score.std()))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8355394797598892\n",
      "\n",
      "Test set score: 0.801477377654663\n",
      "\n",
      "Cross Validation:\n",
      "    0.71 (+/- 0.01)\n",
      "[0.71889401 0.70967742 0.70967742 0.70506912 0.71198157 0.7235023\n",
      " 0.70967742 0.70506912 0.71658986 0.71889401]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2') # No need to specify l2 as it's the default. But we put it for demonstration.\n",
    "lr.fit(X_train_tfidf_csr, y_train)\n",
    "\n",
    "print('Training set score:', lr.score(X_train_tfidf_csr, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test_tfidf_csr, y_test))\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=10, random_state=1337)\n",
    "\n",
    "score = cross_val_score(lr, X_test_tfidf_csr, y_test, cv=split, scoring='accuracy')\n",
    "print(\"\\nCross Validation:\\n    %0.2f (+/- %0.2f)\" % (score.mean(), score.std()))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8356164383561644\n",
      "\n",
      "Test set score: 0.7705447830101569\n",
      "\n",
      "Cross Validation:\n",
      "    0.71 (+/- 0.01)\n",
      "[0.71889401 0.70967742 0.70967742 0.70506912 0.71198157 0.7235023\n",
      " 0.70967742 0.70506912 0.71658986 0.71889401]\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train_tfidf_csr, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train_tfidf_csr, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test_tfidf_csr, y_test))\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=10, random_state=1337)\n",
    "\n",
    "score = cross_val_score(lr, X_test_tfidf_csr, y_test, cv=split, scoring='accuracy')\n",
    "print(\"\\nCross Validation:\\n    %0.2f (+/- %0.2f)\" % (score.mean(), score.std()))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 3000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "blakewords = bag_of_words(blake_doc)\n",
    "paradisewords = bag_of_words(paradise_doc)\n",
    "leaveswords = bag_of_words(leaves_doc)\n",
    "hiawathawords = bag_of_words(hiawatha_doc)\n",
    "endymionwords = bag_of_words(endymion_doc)\n",
    "odysseywords = bag_of_words(odyssey_doc)\n",
    "burnswords = bag_of_words(burns_doc)\n",
    "seawords = bag_of_words(sea_doc)\n",
    "beowulfwords = bag_of_words(beowulf_doc)\n",
    "sapphowords = bag_of_words(sappho_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(blakewords + paradisewords + \n",
    "                   leaveswords + hiawathawords + \n",
    "                   endymionwords + odysseywords +\n",
    "                   burnswords + seawords +\n",
    "                   beowulfwords +sapphowords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "start_time = time.time()\n",
    "print('Processing...')\n",
    "\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "\n",
    "t= round((time.time() - start_time),4)\n",
    "print(\"\\n -- %s seconds --\\n\" % t)\n",
    "\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#increase common words\n",
    "#explore td=idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and the outcome.\n",
    "y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Normalize the data.\n",
    "X_norm = normalize(X)\n",
    "\n",
    "# Reduce it to two components.\n",
    "X_pca = PCA(int(X_norm.shape[0]/2)).fit_transform(X_norm)\n",
    "\n",
    "# Calculate predicted values.\n",
    "y_pred = KMeans(n_clusters=10, random_state=42).fit_predict(X_pca)\n",
    "\n",
    "# Plot the solution.\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred, alpha = 0.5)\n",
    "plt.show()\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Comparing k-means clusters against the data:')\n",
    "print(pd.crosstab(y_pred, y))\n",
    "\n",
    "print(\"--- %s seconds for model fit ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot true values\n",
    "start_time = time.time()\n",
    "\n",
    "# Normalize the data.\n",
    "X_norm = normalize(X)\n",
    "\n",
    "#pca = PCA(2).fit(X_norm)\n",
    "# Reduce it to two components.\n",
    "X_pca = PCA(int(X_norm.shape[0]/2)).fit_transform(X_norm)\n",
    "\n",
    "# Calculate predicted values.\n",
    "y_pred = KMeans(n_clusters=10, random_state=42).fit_predict(X_pca)\n",
    "\n",
    "labels = y.map(lambda x: 0 if x == \"Milton\" else (1 if x == \"Blake\" else 2))\n",
    "# Plot the solution.\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, alpha = 0.5)\n",
    "plt.show()\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Comparing k-means clusters against the data:')\n",
    "print(pd.crosstab(y_pred, y))\n",
    "\n",
    "print(\"--- %s seconds to model ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Each batch will be made up of 200 data points.\n",
    "minibatchkmeans = MiniBatchKMeans(\n",
    "    init='random',\n",
    "    n_clusters=10,\n",
    "    batch_size=200)\n",
    "minibatchkmeans.fit(X_pca)\n",
    "\n",
    "# Add the new predicted cluster memberships to the data frame.\n",
    "predict_mini = minibatchkmeans.predict(X_pca)\n",
    "\n",
    "# Check the MiniBatch model against our earlier one.\n",
    "print('Comparing k-means and mini batch k-means solutions:')\n",
    "print(pd.crosstab(predict_mini, y_pred))\n",
    "\n",
    "print(\"--- %s seconds to model ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca(reduce by half then only plot first 2) or lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify = y,\n",
    "                                                    random_state=0)\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=5, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('NBB', BernoulliNB()))\n",
    "models.append(('RFC', ensemble.RandomForestClassifier()))\n",
    "models.append(('KNN', neighbors.KNeighborsClassifier()))\n",
    "models.append(('DTC', tree.DecisionTreeClassifier()))\n",
    "models.append(('GNB', GaussianNB()))\n",
    "models.append(('SVC', SVC()))\n",
    "models.append(('GBC', ensemble.GradientBoostingClassifier()))\n",
    "models.append(('ABC', ensemble.AdaBoostClassifier()))\n",
    "models.append(('ETC', ensemble.ExtraTreesClassifier()))\n",
    "models.append(('QDA', QuadraticDiscriminantAnalysis()))\n",
    "#models.append(('GMM', GaussianMixture()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    split = StratifiedShuffleSplit(n_splits=10, random_state=1337)\n",
    "    model = model.fit(X_train,y_train)\n",
    "    cv_results = cross_val_score(model, X_test, y_test, cv=split, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=5, random_state=1337)\n",
    "\n",
    "score = cross_val_score(rfc, X_test, y_test, cv=split, scoring='accuracy')\n",
    "print(\"\\nCross Validation:\\n    %0.2f (+/- %0.2f)\" % (score.mean(), score.std()))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2') # No need to specify l2 as it's the default. But we put it for demonstration.\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=5, random_state=1337)\n",
    "\n",
    "score = cross_val_score(lr, X_test, y_test, cv=split, scoring='accuracy')\n",
    "print(\"\\nCross Validation:\\n    %0.2f (+/- %0.2f)\" % (score.mean(), score.std()))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=5, random_state=1337)\n",
    "\n",
    "score = cross_val_score(clf, X_test, y_test, cv=split, scoring='accuracy')\n",
    "print(\"\\nCross Validation:\\n    %0.2f (+/- %0.2f)\" % (score.mean(), score.std()))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
